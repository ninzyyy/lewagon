{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will create the necessary functions to go through the steps of a single Gradient Descent Epoch. You will then combine the functions and create a loop through the entire Gradient Descent procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import for you the following dataset of ingredients with their mineral content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aliment</th>\n",
       "      <th>zinc</th>\n",
       "      <th>phosphorus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Durum wheat pre-cooked. whole grain. cooked. u...</td>\n",
       "      <td>0.120907</td>\n",
       "      <td>0.193784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asian noodles. plain. cooked. unsalted</td>\n",
       "      <td>0.047859</td>\n",
       "      <td>0.060329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rice. brown. cooked. unsalted</td>\n",
       "      <td>0.156171</td>\n",
       "      <td>0.201097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rice. cooked. unsalted</td>\n",
       "      <td>0.065491</td>\n",
       "      <td>0.045704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rice. parboiled. cooked. unsalted</td>\n",
       "      <td>0.025189</td>\n",
       "      <td>0.045704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             aliment      zinc  phosphorus\n",
       "0  Durum wheat pre-cooked. whole grain. cooked. u...  0.120907    0.193784\n",
       "1             Asian noodles. plain. cooked. unsalted  0.047859    0.060329\n",
       "2                      Rice. brown. cooked. unsalted  0.156171    0.201097\n",
       "3                             Rice. cooked. unsalted  0.065491    0.045704\n",
       "4                  Rice. parboiled. cooked. unsalted  0.025189    0.045704"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/04-Under-the-Hood/gradient_descent_ingredients_zinc_phosphorous.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 We can visualize a somewhat Linear relationship between the `Phosphorus` and `Zinc`.   \n",
    "\n",
    "Let's use Gradient Descent to find the line of best fit between them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=data, x='zinc', y='phosphorus');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Create the two `np.Array`\n",
    "- `data_X` for zinc\n",
    "- `data_Y` for phosphorus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "data_X = data['zinc']\n",
    "data_Y = data['phosphorus']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (data_X.shape == (53,))\n",
    "assert (data_Y.shape == (53,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code one Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the exercise, you will define the key functions used to update the parameters during one epoch $\\color {red}{(k)}$ of gradient descent. Recall the formula below\n",
    "\n",
    "$$\n",
    "\\beta_0^{\\color {red}{(k+1)}} = \\beta_0^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_0}(\\beta^{\\color{red}{(k)}})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\beta_1^{\\color {red}{(k+1)}} = \\beta_1^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_1}(\\beta^{\\color {red}{(k)}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} =  a x + b\n",
    "$$\n",
    "\n",
    "❓ Define the hypothesis function of a Linear Regression. Let `a` be the slope and `b` the intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X,a,b):\n",
    "    return a*X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Sum\\ Squares\\ Loss = \\sum_{i=0}^n (y^{(i)} - \\hat{y}^{(i)} )^2\n",
    "$$\n",
    "\n",
    "❓ Define the SSR Loss Function for the Hypothesis Function using the equation above. Reuse the function `h` coded above when writing your new function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss(X,Y,a,b):\n",
    "    return sum((Y - h(X,a,b))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ What would be the total Loss computed on all our ingredients dataset if:\n",
    "- a = 1 \n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.86850698611546"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "loss(data_X, data_Y, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ You should be getting 63.86. If not, something is wrong with your function. Fix it before moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d\\ SSR}{d\\ slope}= \\sum_{i=0}^n -2  x_i (y^{(i)} - \\hat{y}^{(i)} )\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\ SSR}{d\\ intercept}= \\sum_{i=0}^n -2(y^{(i)} - \\hat{y}^{(i)} ) \n",
    "$$\n",
    "\n",
    "❓ Define a function to compute the partial derivatives of the Loss Function relative to parameter `a` and `b` at a given point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>💡 Hint</summary>\n",
    "Again, you must re-use the Hypothesis Function in your new function to compute the predictions at given points.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y,a,b):\n",
    "    d_a = sum(-2*X*(Y - h(X,a,b)))\n",
    "    d_b = sum(-2*(Y - h(X, a, b)))\n",
    "    return d_a, d_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Using your function, what would be the partial derivatives of each parameter if:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48.459065809109, 115.17923733301409)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "gradient(data_X, data_Y, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ You should be getting 48.45 and  115.17. If not, fix your function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Step Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "step\\ size = gradient \\cdot learning\\ rate\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Define a function that calculates the step sizes alongside each parameter (`a`,`b`), according to their derivatives (`d_a`, `d_b`) and a `learning_rate` equal to `0.01` by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps(d_a, d_b, learning_rate = 0.01):\n",
    "    step_a = d_a * learning_rate\n",
    "    step_b = d_b * learning_rate\n",
    "    return (step_a, step_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ What would be the steps (`step_a`, `step_b`) to take for the derivatives computed above for (`a`,`b`) = (1,1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.48450000000000004, 1.1517)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "steps(48.45, 115.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ The steps should be 0.48 for `a` and 1.15 for `b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Update parameters (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "updated\\ parameter = old\\ parameter\\ value - step\\ size\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Define a function that computes the updated parameter values from the old parameter values and the step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(a, b, step_a, step_b):\n",
    "    a_new = a - step_a\n",
    "    b_new = b - step_b\n",
    "    return a_new , b_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 One full epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Using the functions you just created, compute the updated parameters at the end of the first Epoch, had you started with parameters:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.52, -0.1499999999999999)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "update_params(1, 1, 0.48, 1.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ You should be getting the following values:\n",
    "   - updated_a = 0.51\n",
    "   - updated_b = -0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Now that you have the necessary functions for a Gradient Descent, loop through epochs until convergence.\n",
    "\n",
    "- Initialize parameters `a = 1` and  `b = 1`\n",
    "- Consider convergence to be **100 epochs**\n",
    "- Don't forget to start each new epoch with the updated parameters\n",
    "- Append the values for `loss`, `a`, and `b` at each epoch to their corresponding lists called `loss_history`, `a_history` and `b_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "a = 1\n",
    "b = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ What are the parameter values at the end of the 100 epochs? Save them to respective variables `a_100` and `b_100` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_history = []\n",
    "b_history = []\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "for x in range(1,101):\n",
    "    a_history.append(a)\n",
    "    b_history.append(b)\n",
    "    loss_history.append(loss(data_X, data_Y, a, b))\n",
    "    \n",
    "    grad = gradient(data_X, data_Y, a, b)\n",
    "    d_a = grad[0]\n",
    "    d_b = grad[1]\n",
    "    \n",
    "    step = steps(d_a, d_b)\n",
    "    step_a = step[0]\n",
    "    step_b = step[1]\n",
    "    \n",
    "    updated = update_params(a, b, step_a, step_b)\n",
    "    a = updated[0]\n",
    "    b = updated[1]\n",
    "    \n",
    "a_100 = a\n",
    "b_100 = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_100: 0.7686715106203741\n",
      "b_100: 0.007089321156223888\n",
      "a_history: [1, 0.5154093419089101, 0.6475338480410275, 0.6273545046153025, 0.6437520254549169, 0.6504040813696265, 0.6587355634769618, 0.6659889132103665, 0.6728856930511897, 0.6792902431448743, 0.6852746094339561, 0.6908572504746172, 0.6960673673055952, 0.7009292694951631, 0.7054663644254688, 0.7097003182996756, 0.7136513932777129, 0.7173384870685168, 0.7207792374040749, 0.7239901029529684, 0.7269864428989624, 0.7297825902031633, 0.7323919202167937, 0.7348269146495194, 0.7370992212786975, 0.7392197096656721, 0.7411985231504238, 0.7430451273721139, 0.7447683555479074, 0.7463764507265975, 0.7478771052191667, 0.7492774973948976, 0.7505843260190471, 0.7518038422963396, 0.7529418797735559, 0.7540038822442603, 0.7549949297891452, 0.7559197630765584, 0.7567828060394535, 0.7575881870372398, 0.7583397586037577, 0.7590411158758447, 0.7596956137906451, 0.7603063831339264, 0.7608763455161688, 0.7614082273480665, 0.7619045728822924, 0.7623677563839117, 0.7627999934876603, 0.7632033517964163, 0.7635797607715635, 0.7639310209625543, 0.7642588126198245, 0.7645647037322585, 0.764850157527652, 0.7651165394720509, 0.7653651238014482, 0.7655970996170813, 0.7658135765734894, 0.7660155901865369, 0.7662041067867961, 0.7663800281419818, 0.7665441957705504, 0.7666973949670972, 0.7668403585588065, 0.7669737704109253, 0.7670982686980273, 0.7672144489567164, 0.767322866934371, 0.7674240412475576, 0.7675184558628291, 0.7676065624117753, 0.7676887823513988, 0.7677655089801518, 0.7678371093192758, 0.7679039258684452, 0.7679662782441116, 0.7680244647083868, 0.7680787635957776, 0.7681294346445964, 0.7681767202394173, 0.7682208465705214, 0.7682620247158759, 0.7683004516508241, 0.7683363111903151, 0.7683697748681817, 0.7684010027576711, 0.7684301442371535, 0.7684573387046733, 0.7684827162447575, 0.7685063982506751, 0.7685284980051201, 0.7685491212220988, 0.768568366552612, 0.7685863260565506, 0.7686030856430646, 0.7686187254815076, 0.7686333203849272, 0.7686469401679322, 0.7686596499806506]\n",
      "b_history: [1, -0.15179237333014095, 0.12533516894846775, 0.05199049289780795, 0.06505355055558826, 0.05723081117298015, 0.05484465067703286, 0.05141136896993306, 0.048503723903676875, 0.04571760522610792, 0.043135494193196, 0.04072151480231095, 0.038469895918032404, 0.03636844869256201, 0.03440746910002934, 0.0325774913062546, 0.030869782620625875, 0.02927616907688423, 0.027789028902534416, 0.026401248063157082, 0.025106188196032483, 0.023897654373459712, 0.0227698655877463, 0.02171742706873573, 0.02073530448510275, 0.019818799860915354, 0.018963529103333352, 0.018165401031228105, 0.017420597805074732, 0.01672555666433983, 0.01607695288504496, 0.015471683875974848, 0.014906854337455942, 0.01437976241171276, 0.013887886758551614, 0.013428874494549343, 0.013000529938053344, 0.012600804106155234, 0.012227784913396673, 0.011879688025323446, 0.011554848323134895, 0.011251711938601298, 0.010968828821146711, 0.0107048458015434, 0.010458500119036969, 0.01022861338094014, 0.010014085925800241, 0.009813891563177196, 0.009627072664869153, 0.009452735584105611, 0.009290046380794762, 0.00913822683237729, 0.00899655071120516, 0.008864340310636706, 0.00874096320323255, 0.0086258292155434, 0.008518387605020179, 0.008418124425541067, 0.008324560068954136, 0.008237246970875649, 0.008155767469769034, 0.008079731809064225, 0.008008776272760023, 0.007942561445591509, 0.007880770589439498, 0.007823108128216441, 0.0077692982339802865, 0.007719083507513743, 0.007672223747056896, 0.007628494799304215, 0.007587687487168746, 0.007549606609185094, 0.00751407000576485, 0.007480907687837455, 0.007449961023708844, 0.0074210819802479734, 0.007394132414771373, 0.007368983414238865, 0.00734551467859882, 0.007323613945333814, 0.0073031764524539395, 0.0072841044373683425, 0.007266306669238821, 0.007249698012577683, 0.007234199020002866, 0.007219735552202036, 0.007206238423287667, 0.00719364306984674, 0.00718188924210259, 0.007170920715710015, 0.007160685022807025, 0.0071511332010349175, 0.007142219559327341, 0.007133901459347379, 0.007126139111527442, 0.007118895384736102, 0.007112135628661726, 0.007105827508062801, 0.007099940848092782, 0.0070944474899586885]\n",
      "loss_history: [63.86850698611546, 4.956802678221296, 1.3935754675861152, 1.168692944107339, 1.146372419359988, 1.1373957745671932, 1.1302092310311778, 1.123988929707583, 1.1185743402972528, 1.1138592422651652, 1.1097531588184455, 1.1061774212625193, 1.1030635291786142, 1.1003518301665027, 1.097990376609746, 1.0959339304938414, 1.0941430967865544, 1.0925835687530439, 1.0912254707468423, 1.0900427858865411, 1.0890128576542648, 1.0881159558681284, 1.0873348987140807, 1.086654723596451, 1.0860624005017252, 1.085546582384519, 1.0850973877939207, 1.0847062115760018, 1.0843655600261641, 1.084068907333361, 1.0838105705660996, 1.0835856008053721, 1.0833896883389507, 1.0832190801008683, 1.0830705077744964, 1.0829411251818863, 1.0828284537599662, 1.082730335079072, 1.0826448894942342, 1.0825704801371003, 1.082505681558687, 1.0824492524222709, 1.0824001117232829, 1.082357318080665, 1.082320051702973, 1.0822875986837555, 1.0822593373253533, 1.082234726229127, 1.0822132939239704, 1.0821946298344025, 1.0821783764152357, 1.0821642223021395, 1.0821518963468735, 1.0821411624229493, 1.0821318149021812, 1.0821236747155065, 1.0821165859225796, 1.0821104127244494, 1.0821050368620782, 1.0821003553508697, 1.0820962785078119, 1.0820927282334272, 1.0820896365156387, 1.0820869441268686, 1.0820845994894248, 1.0820825576874384, 1.082080779606412, 1.082079231183912, 1.0820778827570383, 1.0820767084941787, 1.082075685900154, 1.082074795385282, 1.082074019890103, 1.082073344558571, 1.08207275645346, 1.0820722443085284, 1.0820717983126906, 1.082071409922072, 1.0820710716963313, 1.082070777156128, 1.0820705206589991, 1.0820702972912672, 1.0820701027739101, 1.0820699333805877, 1.0820697858662582, 1.0820696574050102, 1.0820695455359313, 1.0820694481159618, 1.082069363278842, 1.0820692893993602, 1.0820692250622181, 1.082069169034919, 1.0820691202441548, 1.0820690777552497, 1.082069040754249, 1.0820690085323301, 1.0820689804722234, 1.0820689560363848, 1.082068934756701, 1.0820689162255201]\n"
     ]
    }
   ],
   "source": [
    "print(f'a_100: {a_100}')\n",
    "print(f'b_100: {b_100}')\n",
    "print(f'a_history: {a_history}')\n",
    "print(f'b_history: {b_history}')\n",
    "print(f'loss_history: {loss_history}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/ninaadkalla/.pyenv/versions/3.10.6/envs/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/ninaadkalla/code/ninzyyy/data-batch-gradient-descent/tests\n",
      "plugins: asyncio-0.19.0, anyio-3.6.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_descent.py::TestDescent::test_a \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 50%]\u001b[0m\n",
      "test_descent.py::TestDescent::test_b \u001b[32mPASSED\u001b[0m\u001b[32m                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "💯 You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/descent.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed descent step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 🧪 Test your code\n",
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('descent',\n",
    "                         a_100=a_100,\n",
    "                         b_100=b_100)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 031b1a6] Completed descent step\n",
      " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
      " create mode 100644 tests/descent.pickle\n",
      "Enumerating objects: 13, done.\n",
      "Counting objects: 100% (13/13), done.\n",
      "Delta compression using up to 10 threads\n",
      "Compressing objects: 100% (12/12), done.\n",
      "Writing objects: 100% (13/13), 4.88 KiB | 4.88 MiB/s, done.\n",
      "Total 13 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), done.\u001b[K\n",
      "To github.com:ninzyyy/data-batch-gradient-descent.git\n",
      " * [new branch]      master -> master\n"
     ]
    }
   ],
   "source": [
    "!git add tests/descent.pickle\n",
    "!git commit -m 'Completed descent step'\n",
    "!git push origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Wrap this iterative approach into a method `gradient_descent()` which returns your `new_a`, `new_b` and `history`, a dictionary containing these lists: \n",
    "- `loss_history`\n",
    "- `a_history`\n",
    "- `b_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, a_init=1, b_init=1, learning_rate=0.001, n_epochs=100):\n",
    "    pass  # YOUR CODE HERE\n",
    "    return a, b, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Plot the line of best fit through Zinc and Phosphorus using the parameters of your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize your descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎯 Our goal is to plot our loss function and the descent steps on a 2D surface using matplotlib [contourf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Start by creating the data we need for the plot\n",
    "- `range_a`: a range of 100 values for `a` equally spaced between -1 and 1\n",
    "- `range_b`: a range of 100 values for `b` equally spaced between -1 and 1 \n",
    "- `Z`: a 2D-array where each element `Z[j,i]` is equal to the value of the loss function at `a` = `range_a[i]` and `b` = `range_b[j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Now, plot in one single subplot:\n",
    "- your gradient as a 2D-surface using matplotlib [contourf](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contourf.html) with 3 parameters\n",
    "- all historical (a,b) points as a scatterplot with red dots to visualize your gradient descent!\n",
    "\n",
    "Change your learning rate and observe its impact on the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ [optional] What about 3D? Try to plot the same data on a [plot.ly 3D contour plot](https://plotly.com/python/3d-surface-plots/) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'range_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ninaadkalla/code/ninzyyy/data-batch-gradient-descent/Gradient-Descent.ipynb Cell 66\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ninaadkalla/code/ninzyyy/data-batch-gradient-descent/Gradient-Descent.ipynb#Y116sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgraph_objects\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgo\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ninaadkalla/code/ninzyyy/data-batch-gradient-descent/Gradient-Descent.ipynb#Y116sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m surface \u001b[39m=\u001b[39m go\u001b[39m.\u001b[39mSurface(x\u001b[39m=\u001b[39mrange_a, y\u001b[39m=\u001b[39mrange_b, z\u001b[39m=\u001b[39mZ)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ninaadkalla/code/ninzyyy/data-batch-gradient-descent/Gradient-Descent.ipynb#Y116sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m scatter \u001b[39m=\u001b[39m go\u001b[39m.\u001b[39mScatter3d(x\u001b[39m=\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m], y\u001b[39m=\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m], z\u001b[39m=\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmarkers\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ninaadkalla/code/ninzyyy/data-batch-gradient-descent/Gradient-Descent.ipynb#Y116sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m fig \u001b[39m=\u001b[39m go\u001b[39m.\u001b[39mFigure(data\u001b[39m=\u001b[39m[surface, scatter])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'range_a' is not defined"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "surface = go.Surface(x=range_a, y=range_b, z=Z)\n",
    "scatter = go.Scatter3d(x=history['a'], y=history['b'], z=history['loss'], mode='markers')\n",
    "fig = go.Figure(data=[surface, scatter])\n",
    "\n",
    "#fig.update_layout(title='Loss Function', autosize=False, width=500, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Plot the history of the `loss` values as a function of the number of `epochs`. Try with multiple variations of `learning_rate` from 0.001 to 0.01 and make sure to understand the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. With Sklearn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Using Sklearn, train a Linear Regression model on the same data. Compare its parameters to the ones computed by your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They should be almost identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏁 Congratulation! Please, push your exercise when you are done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
