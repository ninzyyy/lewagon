{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqNDta-xZ2u4"
   },
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp81GSFN7uHY"
   },
   "source": [
    "* ü§Ø Tech companies and university labs have more computational resources than we do\n",
    "* üòé Let them train their super complex models on millions of images, and then re-use their kernels for our own CNNs!\n",
    "\n",
    "üéØ **<u>Goal:</u>**\n",
    "* ‚òÑÔ∏è Use a **Pretrained Neural Network** $ \\Leftrightarrow $ **Transfer learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR9-yIauZ2vA"
   },
   "source": [
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieSm6iw9HizE"
   },
   "source": [
    "Repeat the same process from the last challenge to upload your challenge folder and open your notebook:\n",
    "\n",
    "1. access your [Google Drive](https://drive.google.com/)\n",
    "2. go into the Colab Notebooks folder\n",
    "3. drag and drop this challenge's folder into it\n",
    "4. right-click the notebook file and select `Open with` $\\rightarrow$ `Google Colaboratory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YfeVhhBZ2vC"
   },
   "source": [
    "Don't forget to enable GPU acceleration!\n",
    "\n",
    "`Runtime` $\\rightarrow$ `Change runtime type` $\\rightarrow$ `Hardware accelerator` $\\rightarrow$ `GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoRPmZtQZ2vE"
   },
   "source": [
    "When this is done, run the cells below and get to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21915,
     "status": "ok",
     "timestamp": 1660232102685,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "_q8uOTzh5vwJ",
    "outputId": "f9f36c20-d391-4114-efdb-cd9a38751f5d"
   },
   "outputs": [],
   "source": [
    "# Mount GDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1660232103017,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "7HCO04lsIrcd"
   },
   "outputs": [],
   "source": [
    "# Put Colab in the context of this challenge\n",
    "import os\n",
    "\n",
    "# os.chdir allows you to change directories, like cd in the Terminal\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/data-transfer-learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0MdAwhGJdSR"
   },
   "source": [
    "You are now good to go, proceed with the challenge! Don't forget to copy everything back to your PC to upload to Kitt üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsdd1Jv7Z2vJ"
   },
   "source": [
    "## (1) What is a Pre-Trained Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shgjzVW4Z2vJ"
   },
   "source": [
    "* Convolutions are mathematical operations designed to detect specific patterns in input images and use them to classify the images. \n",
    "* One could imagine that these patterns are not 100% specific to one task but to the input images. \n",
    "\n",
    "üöÄ **Why not re-use these kernels - whose weights have already been optimized - somewhere else?** \n",
    "- The expectation is that the trained kernels could also help us perform another classification task.\n",
    "- We are trying to ***transfer*** the knowledge of a trained CNN to a new classification task.\n",
    "\n",
    "\n",
    "üí™ Transfer Learning has two main advantages:\n",
    "- It takes less time to train a pre-trained model since we are not going to update all the weights but only some of them\n",
    "- You benefit from state-of-the-art architectures that have been trained on complex images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOkQIQj4Z2vJ"
   },
   "source": [
    "## (2) Introduction to  VGG16 \n",
    "\n",
    "üìö ***Reading Section, no code***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAkNOgtqF7S_"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this exercise, we will use the <a href=\"https://neurohive.io/en/popular-networks/vgg16/\">**`VGG-16 Neural Network`**</a>.\n",
    "\n",
    "> VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper ‚ÄúVery Deep Convolutional Networks for Large-Scale Image Recognition‚Äù. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3√ó3 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU‚Äôs.\n",
    "\n",
    "VGG16 is a well-known architecture that has been trained on the <a href=\"https://www.image-net.org/\">**`ImageNet dataset`**</a> which is a very large database of images which belong to different categories. \n",
    "\n",
    "üëâ This architecture already learned which kernels are the best for extracting features from the images found in the `ImageNet dataset`.\n",
    "\n",
    "üëâ As you can see in the illustration, the VGG16 involves millions of parameters you don't want to retrain yourself.\n",
    "\n",
    "\n",
    "<center><img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\" width=400></center>\n",
    "\n",
    "‚ùì How does it work in practice ‚ùì\n",
    "\n",
    "* The first layers are not specialized for the particular task the VGG16 CNN was trained on\n",
    "* Only the last dense layer is a \"classification layers\" that can be preceded with a couple of dense layers...  Therefore, we will: \n",
    "    1. Load the existing VGG16 network\n",
    "    2. Remove the last fully connected layers\n",
    "    3. Replace them with some new fully-connected layers (whose weights are randomly set)\n",
    "    4. Train these last layers on a specific classification task. \n",
    "\n",
    "üòÉ Your role is to train only the last layers for your particular problem.\n",
    "\n",
    "ü§ì We will use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16\">**`tensorflow.keras.applications.VGG16`**</a>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-S858KRF7TA"
   },
   "source": [
    "## (3) Data loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzLvo4N5Z2vL"
   },
   "source": [
    "You have two options to load the data into Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6vPgeK2Z2vL"
   },
   "source": [
    "### (Option 1) Loading the data directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6FxEU-HZ2vL"
   },
   "source": [
    "* You can first get the data onto google Colab thanks to:\n",
    "\n",
    "`!wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip`,\n",
    "\n",
    "* and then run \n",
    "\n",
    "`!unzip flowers-dataset.zip`\n",
    "\n",
    "*This is a very easy option to load the data into your working directory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ID413NdhZ2vL"
   },
   "source": [
    "### (Option 2) Adding the data to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsnMYzYgZ2vM"
   },
   "source": [
    "* You can first download the data from `https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip`. \n",
    "* Then you have to add it to your Google Drive in a folder called `Deep_learning_data` (for instance)\n",
    "* And run the following code in the notebook: \n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "```\n",
    "\n",
    "* The previous code will ask you to go to a given webpage where you can copy a temporary key\n",
    "* Paste it in the cell that will appear in your Colab Notebook\n",
    "* You can now load the data on your Google Colab Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yrUUBluZ2vM"
   },
   "source": [
    "### Option 1 or Option 2 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdS5hErJF7TB"
   },
   "source": [
    "* Why choosing option 2 over the option 1? \n",
    "    * ‚úÖ The combo Colab + Drive can be interesting if you work within a project team, and need to update the data from time to time. \n",
    "    * ‚úÖ By doing this, you can share the same data folder with your teammates, and be sure that everyone has the same dataset at any time, even though someone changes it. \n",
    "    * ‚ùå Google Colab has now access to your Google Folder..., which you may or may not be in favor of, depending on your sensibilities...\n",
    "\n",
    "---\n",
    "\n",
    "‚ùì **Question: Loading your dataset** ‚ùì \n",
    "    \n",
    "Use one of the above methods to load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKw_1TjOF7TC"
   },
   "outputs": [],
   "source": [
    "option_1 = True # Choose here\n",
    "\n",
    "if option_1:\n",
    "    !wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip\n",
    "    !unzip flowers-dataset.zip\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onPwIzQzF7TE"
   },
   "source": [
    "‚ùì **Question:Train/Val/Test split** ‚ùì \n",
    "\n",
    "Use the following method to create \n",
    "`X_train, y_train, X_val, y_val, X_test, y_test, num_classes` depending on the `loading_method` you have used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_flowers_data(loading_method):\n",
    "    if loading_method == 'colab':\n",
    "        data_path = '/content/drive/My Drive/Deep_learning_data/flowers'\n",
    "    elif loading_method == 'direct':\n",
    "        data_path = 'flowers/'\n",
    "    classes = {'daisy':0, 'dandelion':1, 'rose':2}\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for (cl, i) in classes.items():\n",
    "        images_path = [elt for elt in os.listdir(os.path.join(data_path, cl)) if elt.find('.jpg')>0]\n",
    "        for img in tqdm(images_path[:300]):\n",
    "            path = os.path.join(data_path, cl, img)\n",
    "            if os.path.exists(path):\n",
    "                image = Image.open(path)\n",
    "                image = image.resize((256, 256))\n",
    "                imgs.append(np.array(image))\n",
    "                labels.append(i)\n",
    "\n",
    "    X = np.array(imgs)\n",
    "    num_classes = len(set(labels))\n",
    "    y = to_categorical(labels, num_classes)\n",
    "\n",
    "    # Finally we shuffle:\n",
    "    p = np.random.permutation(len(X))\n",
    "    X, y = X[p], y[p]\n",
    "\n",
    "    first_split = int(len(imgs) /6.)\n",
    "    second_split = first_split + int(len(imgs) * 0.2)\n",
    "    X_test, X_val, X_train = X[:first_split], X[first_split:second_split], X[second_split:]\n",
    "    y_test, y_val, y_train = y[:first_split], y[first_split:second_split], y[second_split:]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALL load_flowers_data WITH YOUR PREFERRED METHOD HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm2IsMmGF7TH"
   },
   "source": [
    "‚ùì **Question: Exploring the images** ‚ùì\n",
    "\n",
    "Check the images' shapes and plot a few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdMfP6dyF7TJ"
   },
   "source": [
    "## (4) A CNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njYcWjw1F7TJ"
   },
   "source": [
    "First, let's build our own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wneAeLYdF7TK"
   },
   "source": [
    "‚ùì **Questions** ‚ùì \n",
    "\n",
    "1. <u>CNN Architecture and compiler:</u> Create a CNN with your own architecture and a function `load_own_model` that will be able to generate it. Some advice:\n",
    "    - Incorporate the Rescaling Layer in your Sequential architecture\n",
    "    - Add three Conv2D/MaxPooling2D combinations with an increasing number of channels and a decreasing size of kernels for example (be creative, that is not a rule of thumb, mastering CNN is an art)\n",
    "    - Don't forget the Flatten layer and some hidden layers\n",
    "    - Finish with the predictive layer\n",
    "    - Compile your CNN model accordingly\n",
    "  \n",
    "  \n",
    "2. <u>Training and comparison</u>:\n",
    "    - Train your CNN\n",
    "    - Compare its performance to a baseline accuracy\n",
    "\n",
    "<details>\n",
    "    <summary><i>Recommended architecture:</i></summary>\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Notice this cool new layer that \"pipe\" your rescaling within the architecture\n",
    "model.add(Rescaling(1./255, input_shape=(256,256,3)))\n",
    "\n",
    "# Lets add 3 convolution layers, with relatively large kernel size as our pictures are quite big too\n",
    "model.add(layers.Conv2D(16, kernel_size=10, activation='relu'))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "model.add(layers.Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "model.add(layers.Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "```\n",
    "\n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = res[-1]\n",
    "print(f\"test_accuracy = {round(test_accuracy,2)*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DC2SFwUZ2vR"
   },
   "source": [
    "ü•° <b><u>Takeaways from building your own CNN</u></b>:\n",
    "* On an \"easy dataset\" like the MNIST, it is now easy to reach a decent accuracy. But for a more complicated problem like classifying flowers, it already becomes more challenging. Take a few minutes to play with the following link before moving on to Transfer Learning\n",
    "    * [PoloClub/CNN-Explainer](https://poloclub.github.io/cnn-explainer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sx8V9sny7cLP"
   },
   "source": [
    "## (5) Using a pre-trained CNN = Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxhI8bluZ2vR"
   },
   "source": [
    "As we said in the beginning, tech companies and university labs have more computational resources than we do.\n",
    "\n",
    "üî• The [**Visual Geometry Group**](https://www.robots.ox.ac.uk/~vgg/data/) *(Oxford University, Department of Science and Engineering)* became famous for some of their **Very Deep Convolutional Neural Networks**: the [**VGG16**](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    "\n",
    "Take 7 minutes of your time to watch this incredible video of Convolutional Layers created by Dimitri Dmitriev.\n",
    "\n",
    "* üì∫ **[VGG16 Neural Network Visualization](https://www.youtube.com/watch?v=RNnKtNrsrmg)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZldqCInZ2vS"
   },
   "source": [
    "### (5.1) Load VGG16 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr6m5eKs9s54"
   },
   "source": [
    "‚ùì **Question: loading the VGG16** ‚ùì \n",
    "\n",
    "* Write a first function `load_model()` that loads the pretrained VGG-16 model from `tensorflow.keras.applications.vgg16`. Have a look at the documentation üìö  [tf/keras/applications/VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16)üìö\n",
    "\n",
    "* We will **load the VGG16 model** the following way:\n",
    "    - ü§Ø Let's use the **weights** learned on the [**imagenet dataset**](https://www.image-net.org/download.php) (14M pictures with 20k labels)\n",
    "    - The **`input_shape`** corresponds to the input shape of your images \n",
    "        - Note: *You have to resize them down to a consistent shape if they have different height/widths/channels*\n",
    "    - The **`include_top`** argument should be set to `False`: \n",
    "        - to avoid loading the weights of the fully-connected layers of the VGG16\n",
    "        - and also remove the last layer of the VGG16 which was specifically trained on `imagenet`\n",
    "\n",
    "<i><u>Remark:</u></i> Do not change the default value of the other arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "def load_model():\n",
    "    \n",
    "    pass  # YOUR CODE HERE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3psG3JGF7TO"
   },
   "source": [
    "‚ùì **Question: number of parameters in the VGG16** ‚ùì \n",
    "\n",
    "Look at the architecture of the model using ***.summary()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtBpKLegF7TO",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1IAwxRVF7TO"
   },
   "source": [
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\">\n",
    "\n",
    "üí™ Impressive, right? Two things to notice:\n",
    "- It ends with a combo Conv2D/MaxPooling2D \n",
    "- The `layers.Flatten` and the `layers.Dense` are not there yet, we need to add them.\n",
    "- There are more than 14,000,000 parameters, which is a lot... \n",
    "    - We could fine-tune them, i.e. update them as we will update the weights of the dense layers, but it will take a lot of time....\n",
    "    - For this reason, we will inform the model that the layers before the flattening will be set non-trainable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehMaUN34Z2vT"
   },
   "source": [
    "‚ùì **Question: deactivating the training of the VGG16 paramters** ‚ùì \n",
    "\n",
    "* Write a first function which:\n",
    "    - takes the previous model as the input\n",
    "    - sets the first layers to be non-trainable, by applying **`model.trainable = False`**\n",
    "    - returns the model.\n",
    "\n",
    "* Then inspect the summary of the model to check that the parameters are no longer trainable, they were set to be **`non-trainable`**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_nontrainable_layers(model):\n",
    "    \n",
    "    pass  # YOUR CODE HERE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4yedT2VF7TP"
   },
   "source": [
    "‚ùì **Question: chaining the pretrained convolutional layers of VGG16 with our own dense layers** ‚ùì \n",
    "\n",
    "We will write a function that adds flattening and dense layers after the convolutional layers. To do so, we cannot directly use the classic `layers.Sequential()` instantiation.\n",
    "\n",
    "For that reason, we will discover another way here. The idea is that we define each layer (or group of layers) separately. Then, we concatenate them. Have a look at this example: \n",
    "\n",
    "---\n",
    "```python\n",
    "base_model = load_model()\n",
    "base_model = set_nontrainable_layers(base_model)\n",
    "flattening_layer = layers.Flatten()\n",
    "dense_layer = layers.Dense(SOME_NUMBER_1, activation='relu')\n",
    "prediction_layer = layers.Dense(SOME_NUMBER_2, activation='APPROPRIATE_ACTIVATION')\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  flattening_layer,\n",
    "  dense_layer,\n",
    "  prediction_layer\n",
    "])\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "* The first line loads a group of layers which is the previous VGG-16 model. \n",
    "* Then, we set these layers to be non-trainable.\n",
    "* Eventually, we can instantiate as many layers as we want.\n",
    "* Finally, we use the `Sequential` with the sequence of layers that will correspond to our overall neural network. \n",
    "\n",
    "Replicate the following steps by adding:\n",
    "* a flattening layer\n",
    "* two dense layers (the first with 500 neurons) to the previous VGG-16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def add_last_layers(model):\n",
    "    '''Take a pre-trained model, set its parameters as non-trainable, and add additional trainable layers on top'''\n",
    "    pass  # YOUR CODE HERE\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-24j0psF7TQ"
   },
   "source": [
    "‚ùì **Question: inspect the parameters of a customized VGG16** ‚ùì \n",
    "\n",
    "* Now look at the layers and the parameters of your model. \n",
    "* Note that there is a distinction, at the end, between the **trainable** and **non-trainable parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uqGuxsJF7TR",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxXlnPp5F7TR"
   },
   "source": [
    "‚ùì **Question: building a function that creates a full customized VGG16 and compiles it** ‚ùì \n",
    "\n",
    "* Write a function which builds and compiles your model\n",
    "    * We advise using the _adam_ optimizer with `learning_rate=1e-4`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "def build_model():\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S96qsiKxZ2vU"
   },
   "source": [
    "### (5.2) Back to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbkwOw1eF7TS"
   },
   "source": [
    "üö® The VGG16 model was trained on images which were preprocessed in a specific way. This is the reason why we did _NOT_ normalize them earlier.\n",
    "\n",
    "‚ùì **Question: preprocessing the dataset** ‚ùì \n",
    "\n",
    "Apply the specific processing to the original (non-normalized) images here using the method **`preprocess_input`** that you can import from **`tensorflow.keras.applications.vgg16`**\n",
    "\n",
    "üìö Cf. [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNeJZvtV3YDf",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T7HvbQfZ2vZ"
   },
   "source": [
    "### (5.3)  Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu2H0KZF-EoI"
   },
   "source": [
    "\n",
    "\n",
    "‚ùì **Question: Training the customized VGG16** ‚ùì \n",
    "\n",
    "* Train the model with an Early stopping criterion on the validation accuracy -\n",
    "* Since the validation data is provided use `validation_data` instead of `validation_split`.\n",
    "\n",
    "_As usual, store the results of your training into a `history` variable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grmnNmjeAXcQ",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec_I9JpiAm-W"
   },
   "source": [
    "‚ùì **Question: Looking at the accuracy** ‚ùì \n",
    "\n",
    "Plot the accuracy for both the train set and and the validation set using the usual function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
    "    if axs is not None:\n",
    "        ax1, ax2 = axs\n",
    "    else:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
    "        exp_name = '_' + exp_name\n",
    "    ax1.plot(history.history['loss'], label='train' + exp_name)\n",
    "    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n",
    "    #ax1.set_ylim(0., 2.2)\n",
    "    ax1.set_title('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
    "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
    "    #ax2.set_ylim(0.25, 1.)\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "    return (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESzinGOY6aBc",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3plexlQAtcC"
   },
   "source": [
    "‚ùì **Question: Evaluating the model** ‚ùì\n",
    "\n",
    "Evaluate the customized VGG16 accuracy on the test set. Did we improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ps_9HwUyRVj9",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5T1KvsGZ2va"
   },
   "source": [
    "## (6) (Optional) Improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF39HIb7BSOy"
   },
   "source": [
    "Now, you can try to improve the model's test accuracy. To do that, here are some options you can consider\n",
    "\n",
    "1. **Unfreeze and finetune**: Source: [Google tutorial](https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning) \n",
    ">_Once your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate. This is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting -- keep that in mind. It is critical to only do this step after the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features. It's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to readapt the pretrained weights in an incremental way._\n",
    "\n",
    "\n",
    "1. Add **Data Augmentation** if your model is overfitting. \n",
    "\n",
    "2. If your model is not overfitting, try a more complex model.\n",
    "\n",
    "3. Perform a precise **Grid Search** on all the hyper-parameters: learning_rate, batch_size, data augmentation etc...\n",
    "\n",
    "4. **Change the base model** to more modern one CNN (ResNet, EfficientNet1,... available in the keras library)\n",
    "\n",
    "5. Curate the data: maintaining a sane data set is one of the keys to success.\n",
    "\n",
    "6. Collect more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3UMNBZHZ2vb"
   },
   "source": [
    "## (6.2) Comparing the performances of the CNN, the VGG, and the VGG trained on the augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_aug = res_aug[-1]\n",
    "\n",
    "\n",
    "print(f\"test_accuracy_aug = {round(test_accuracy_aug,2)*100} %\")\n",
    "\n",
    "print(f\"test_accuracy_vgg = {round(test_accuracy_vgg,2)*100} %\")\n",
    "\n",
    "print(f\"test_accuracy = {round(test_accuracy,2)*100} %\")\n",
    "\n",
    "print(f'Chance level: {1./num_classes*100:.1f}%')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "u8gaSAxLZ2vc"
   },
   "source": [
    "---\n",
    "\n",
    "üèÅ **Congratulations** üèÅ \n",
    "\n",
    "1. Download this notebook from your `Google Drive` or directly from `Google Colab` \n",
    "2. Drag-and-drop it from your `Downloads` folder to your local challenge folder  \n",
    "\n",
    "\n",
    "üíæ Don't forget to push your code\n",
    "\n",
    "3. Follow the usual procedure on your terminal inside the challenge folder:\n",
    "      * *git add transfer_learning.ipynb*\n",
    "      * *git commit -m \"I am the god of Transfer Learning\"*\n",
    "      * *git push origin master*\n",
    "\n",
    "*Hint*: To find where this Colab notebook has been saved, click on `File` $\\rightarrow$ `Locate in Drive`.\n",
    "\n",
    "üöÄ If you have time, move on to the **Autoencoders** challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
