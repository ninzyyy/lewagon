{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is classified as: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(f'This image is classified as: {y_train[16]}')\n",
    "plt.imshow(X_train[16], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train_normalized = X_train / 255\n",
    "X_test_normalized = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([60000, 28, 28, 1]), TensorShape([10000, 28, 28, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_expanded = expand_dims(X_train_normalized)\n",
    "X_test_expanded = expand_dims(X_test_normalized)\n",
    "(X_train_expanded.shape, X_test_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "üëâ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4,4), activation='relu', padding='same', input_shape=(28,28,1)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    \n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    ### Model compilation\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 25, 25, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 12, 12, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 10, 10, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 5, 5, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                4010      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,424\n",
      "Trainable params: 5,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "model = initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# of trainable parameters\n",
    "# ((4 * 4 * 1) + 1) * 8 = 136 --> first conv2d\n",
    "# ((3 * 3 * 8) + 1) * 16 = 1168 --> second conv2d\n",
    "# (400 + 1) * 10 = 4010 --> fully connected layer\n",
    "# (10 + 1) * 10 = 110 --> output layer \n",
    "# of total trainable parameters = 5424"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
