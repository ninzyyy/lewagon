{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoYUyLXkZ4p0"
   },
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpRuldPYZ4p2"
   },
   "source": [
    "üéØ **Exercise objectives**\n",
    "- Discover ***autoencoders***\n",
    "- Get a deeper understanding of CNNs\n",
    "\n",
    "<hr>\n",
    "\n",
    "üëâ There exists a very particular architecture in Deep Learning called **`Autoencoders`**. Autoencoders are Neural Network architectures trained to return **outputs that are as similar as possible to the original inputs fed to them**. Why would we do that?  \n",
    "\n",
    "Before answering the question _\"why\"_, let's answer the question _\"how\"_.\n",
    "\n",
    "üë©üèª‚Äçüè´ <u>***How does an autoencoder work ?***</u>\n",
    "\n",
    "There are two parts in an autoencoder: the  **`encoder`** and the **`decoder`**.\n",
    "\n",
    "1. In the encoder, we will make the information flow through different dense layers with a decreasing number of neurons. It will create a **`bottleneck`** where the information is compressed.\n",
    "\n",
    "2. In the decoder, we will try to recreate the original data based on the compressed data.\n",
    "\n",
    "üî• <u>***Why is it powerful or useful?***</u>\n",
    "\n",
    "If it works well, it means two important things:\n",
    "\n",
    "* ‚úÖ We can afford to **compress our dataset** and use a compressed version of it when fitting another Neural Network! \n",
    "\n",
    "* ‚úÖ The **information contained in the bottleneck** - i.e. the data compressed in a low-dimensional layer - **accurately captures the patterns of our dataset** and the autoencoder is able to decode the compressed information!\n",
    "\n",
    "üå† <u>**Applications:**</u>\n",
    "- Image compression\n",
    "- Denoising (cf. Google Pixel phones...)\n",
    "- Image generation!\n",
    "\n",
    "\n",
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/DL/autoencoder.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92EPHYLxZ4p3"
   },
   "source": [
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieSm6iw9HizE"
   },
   "source": [
    "Repeat the same process from the last challenge to upload your challenge folder and open your notebook:\n",
    "\n",
    "1. access your [Google Drive](https://drive.google.com/)\n",
    "2. go into the Colab Notebooks folder\n",
    "3. drag and drop this challenge's folder into it\n",
    "4. right-click the notebook file and select `Open with` $\\rightarrow$ `Google Colaboratory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87wB87mNZ4p4"
   },
   "source": [
    "Don't forget to enable GPU acceleration!\n",
    "\n",
    "`Runtime` $\\rightarrow$ `Change runtime type` $\\rightarrow$ `Hardware accelerator` $\\rightarrow$ `GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRsePK8CZ4p4"
   },
   "source": [
    "When this is done, run the cells below and get to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3621,
     "status": "ok",
     "timestamp": 1660232283037,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "_q8uOTzh5vwJ",
    "outputId": "20ab4161-218f-4d75-d747-ce704b7e271d"
   },
   "outputs": [],
   "source": [
    "# Mount GDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660232265256,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "7HCO04lsIrcd"
   },
   "outputs": [],
   "source": [
    "# Put Colab in the context of this challenge\n",
    "import os\n",
    "\n",
    "# os.chdir allows you to change directories, like cd in the Terminal\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/data-autoencoders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0MdAwhGJdSR"
   },
   "source": [
    "You are now good to go, proceed with the challenge! Don't forget to copy everything back to your PC to upload to Kitt üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fu3kwNPNZ4p7"
   },
   "source": [
    "## (0) The MNIST Dataset\n",
    "\n",
    "In this notebook, we will train an auto-encoder to work on 28x28 grey images from the MNIST dataset, available in Keras. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3664,
     "status": "ok",
     "timestamp": 1660229806107,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "5duLpZCOZ4p7",
    "outputId": "5e568a4a-6efb-47e6-cadf-2d1a6dc454ed"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "print(images_train.shape)\n",
    "print(images_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jF8QY_jZ4p7"
   },
   "outputs": [],
   "source": [
    "# Add a channels for the colors and normalize data\n",
    "X_train = images_train.reshape((60000, 28, 28, 1)) / 255.\n",
    "X_test = images_test.reshape((10000, 28, 28, 1)) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1660229806502,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "vqDfe0h8Z4p7",
    "outputId": "c74a30d6-b034-4ada-d49d-09792f6a7036"
   },
   "outputs": [],
   "source": [
    "# Plot some images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, axs = plt.subplots(1, 10, figsize=(20, 4))\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.axis('off')\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='Greys')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htkNI1BlZ4p7"
   },
   "source": [
    "## (1) The encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VcLZTeqZ4p8"
   },
   "source": [
    "üéÅ First, we built the \"Encoder\" part for you.\n",
    "\n",
    "üëâ  Notice how similar it looks compared to a Convolutional Classifier with **latent_dimension** neurons at the end. However, we using the \"tanh\" activation function in the final dense layer instead of \"relu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dqWxtj0Z4p8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "def build_encoder(latent_dimension):\n",
    "    '''returns an encoder model, of output_shape equals to latent_dimension'''\n",
    "    encoder = Sequential()\n",
    "    \n",
    "    encoder.add(Conv2D(8, (2,2), input_shape=(28, 28, 1), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))\n",
    "\n",
    "    encoder.add(Conv2D(16, (2, 2), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))\n",
    "\n",
    "    encoder.add(Conv2D(32, (2, 2), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))     \n",
    "\n",
    "    encoder.add(Flatten())\n",
    "    encoder.add(Dense(latent_dimension, activation='tanh'))\n",
    "    \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyYuLZByZ4p8"
   },
   "source": [
    "‚ùì **Question: building an encoder** ‚ùì \n",
    "\n",
    "Build your encoder with **`latent_dimension = 2`** and look at the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1660229806960,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "osT4mGsLZ4p8",
    "outputId": "2d0b158c-6df0-4fe4-82db-29be3920bf24",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwQY7i4KZ4p8"
   },
   "source": [
    "## (2) Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVPLjHLtZ4p8"
   },
   "source": [
    "It's your turn to build the decoder this time!\n",
    "\n",
    "We need to build a üî• **`reversed CNN` üî•** that \n",
    "* takes a dense layer as input,\n",
    "* and outputs an image of shape $ (28,28,1) $ similar to our MNIST images. \n",
    "\n",
    "üìö For this purpose, we will use a new layer called <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose\">**`Conv2DTranspose`**</a> üìö\n",
    "    \n",
    "The name of this layer speaks for itself: it performs the opposite of a convolution operation!\n",
    "\n",
    "üí° We will follow this strategy:\n",
    "* Start by reshaping the Dense Input Layer into an Image of shape $(7,7,..)$\n",
    "* Then apply the `Conv2DTranspose` operation with ***strides = 2*** to double the output shape to $(14,14,..)$\n",
    "* then add another Conv2DTranpose layer on top of the first one to make it $(28,28,1)$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "‚ùì **Question: the architecture of a decoder** ‚ùì \n",
    "\n",
    "\n",
    "Define a **`decoding architecture`** in the method below as follows:\n",
    "- a *Dense* layer with:\n",
    "    - $7 \\times 7 \\times 8$ neurons, \n",
    "    - *input_shape* = (latent_dimension, )\n",
    "    - *tanh* activation function. \n",
    "- a *Reshape* layer that reshapes to $(7, 7, 8)$ tensors\n",
    "- a *Conv2DTranspose* with:\n",
    "    - $8$ filters, \n",
    "    - $(2,2)$ kernels, \n",
    "    - strides of $2$, \n",
    "    - padding *same* \n",
    "    - _relu_ activation function\n",
    "- a second Conv2DTranspose layer with:\n",
    "    - $1$ filter,\n",
    "    - $(2,2)$ kernels,\n",
    "    - strides of $2$,\n",
    "    - padding _same_,\n",
    "    - _relu_ activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAyZKVSEZ4p9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "\n",
    "def build_decoder(latent_dimension):\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2twv_e7Z4p9"
   },
   "source": [
    "‚ùì **Question: buiding a decoder** ‚ùì \n",
    "\n",
    "Build your decoder with **`latent_dimension = 2`** and check that it outputs images of same shape than the encoder input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1660229806961,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "nvsK6kW4Z4p9",
    "outputId": "62e76c83-4a06-41ae-c03e-9efd9fadc627",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypHl089KZ4p9"
   },
   "source": [
    "## (3) Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZSJi8BmZ4p9"
   },
   "source": [
    "üéâ We can now **concatenate** both **`the encoder and the decoder`** thanks to the **`Model`** class in Keras, using the **`Functional API`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KP8doE9AZ4p9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "def build_autoencoder(encoder, decoder):\n",
    "    inp = Input((28, 28,1))\n",
    "    encoded = encoder(inp)\n",
    "    decoded = decoder(encoded)\n",
    "    autoencoder = Model(inp, decoded)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmBjTx4DZ4p9"
   },
   "source": [
    "‚ùì **Questions** ‚ùì \n",
    "\n",
    "* Try to understand syntax above üëÜ \n",
    "* Build your autoencoder\n",
    "* Have a look at the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1660229807264,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "nz9CNYMXZ4p-",
    "outputId": "488d2704-6edb-42db-86d1-7cf8b1a6294e",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZr5LnK2Z4p-"
   },
   "source": [
    "‚ùì **Question: Compiling an autoencoder** ‚ùì \n",
    "\n",
    "Define a method which compiles your model. Pick an appropriate loss.\n",
    "\n",
    "<u><i>Think carefully:</i></u> ü§î On which mathematical object are we going to compare *predictions* and the *ground truth* for the computation of the loss function and the metrics?\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "It should compare two images (Black and White in our case), pixel-by-pixel!\n",
    "    \n",
    "The MSE loss seems to be an appropriate loss function for pixel-by-pixel error minimization.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55oM3lFwZ4p-",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sux3UYMWZ4p-"
   },
   "source": [
    "‚ùì **Question: Training an autoencoder** ‚ùì  \n",
    "\n",
    "* Compile your model and fit it with `batch_size = 32` and `epochs = 20`. \n",
    "* What is the label `y_train` in this case?\n",
    "\n",
    "<i>Note:</i> Don't waste your time fighting overfitting in this challenge, you will have time to care about this during the project weeks :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qk13NOy4Z4p-",
    "outputId": "668569e7-0a1f-4c31-8a57-380d23009b2a",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2O9sW6FZ4p_"
   },
   "source": [
    "‚ùì **Question: Encoding the dataset** ‚ùì\n",
    "\n",
    "* Using only the encoder part of the network, encode your dataset and save it under `X_encoded` . \n",
    "    * Each image is now represented by two values (that correspond to the dimension of the latent space, of the bottleneck; aka the `latent_dimension`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-BmhxwhZ4p_",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX_35d9UZ4p_"
   },
   "source": [
    "ü§î Where are we after running the encoder?\n",
    "\n",
    "* Each image was compressed into a 2D space. \n",
    "* Each of these handwritten digit have a given label, between 0 and 9, but the goal here is not to classify these pictures like in the first challenge but to **reconstruct the original image before the compression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReYziVSoZ4p_"
   },
   "source": [
    "‚ùì **Question: Visualizing handwritten digits in the latent space** ‚ùì \n",
    "\n",
    "Scatterplot the encoded data (only a small fraction of the encoded dataset for visibility purposes...)\n",
    "- Each point of the scatter plot  corresponds to an encoded image\n",
    "- Color the dots according to their respective labels (digit representation):\n",
    "    - for instance, all the \"4\"s should be represented by a color on this scatter plot...\n",
    "    - ...while the \"5\" should be represented by another color\n",
    "    - choose a set of [`qualitative colormaps`](https://matplotlib.org/stable/gallery/color/colormap_reference.html)\n",
    "\n",
    "What do you remark about this plot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esW5nugAZ4p_"
   },
   "outputs": [],
   "source": [
    "labels_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAS-vl1OZ4p_",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jmGKIrO6Z4qA"
   },
   "source": [
    "## (4) Application: Image generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1mdTdXS5Z4qA"
   },
   "source": [
    "‚ùì **Questions: Generate new digits** ‚ùì \n",
    "\n",
    "* Let's create some new digits!\n",
    "* Run the following code editing the latent coordinates.\n",
    "* Play with the coordinates start with ones from the graph above.\n",
    "* For example [0.75, 0.75] is the \"zero\" area with latent space (don't forget to experiment outside the boundaries of our original dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_coords = np.array([[0.75, -0.75]]) \n",
    "generated_img = decoder.predict(latent_coords) \n",
    "plt.imshow(generated_img.reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1mdTdXS5Z4qA"
   },
   "source": [
    "‚ùóÔ∏è We can reuse the decoder of auto encoders as one of the simplest forms of generative deep learning. When we enter coordinates that were not in the training dataset **we are creating never seen before digits!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jmGKIrO6Z4qA"
   },
   "source": [
    "## (5) Application: Image denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mdTdXS5Z4qA"
   },
   "source": [
    "‚ùì **Questions: Creating some noise in the dataset** ‚ùì \n",
    "\n",
    "* Let's add some noise to the input data. \n",
    "* Run the following code\n",
    "* Plot some handwritten digits and their noisy versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vTppRiPZ4qA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "noise_factor = 0.5\n",
    "\n",
    "X_train_noisy = X_train + noise_factor * np.random.normal(0., 1., size=X_train.shape)\n",
    "X_test_noisy = X_test + noise_factor * np.random.normal(0., 1., size=X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtREE11sZ4qA",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhm2ETi4Z4qA"
   },
   "source": [
    "‚ùì **Question: decoding the noisy pictures** ‚ùì \n",
    "\n",
    "* Reinitialize your autoencoder (with a latent space of 2) \n",
    "* Train it again, this time using the noisy train dataset instead of the normal train dataset\n",
    "    * *Keep `batch_size = 32` and `epochs = 5`*\n",
    "* What do you expect if you run the autoencoder on the noisy data instead of the original data in terms of performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBt7w1XxZ4qB",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjoWn2ojZ4qB"
   },
   "source": [
    "‚ùì **Question: comparing the noisy test images with the denoised images** ‚ùì \n",
    "\n",
    "For some noisy test images, predict the denoised images and plot the results side by side..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxelVG8JZ4qB",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oluypjaBZ4qB"
   },
   "source": [
    "‚ùì **Question: choosing the \"correct\" latent_dimension** ‚ùì \n",
    "\n",
    "Now, try to evaluate which **`latent_dimension`** is the most suitable in order to have **`the best image reconstruction preprocess`** $ \\Leftrightarrow $ How to remove as much noise as possible in the noisy dataset using the latent dimension?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkanHoBvZ4qB"
   },
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOwthU6KZ4qC"
   },
   "source": [
    "ü•° <b><u>Conclusion</u></b>\n",
    "\n",
    "\n",
    "* It is obvious that:\n",
    "    * if you compress your pictures of size $ 28 \\times 28 $ into a 1D space, you will lose a ton of information. \n",
    "    * if you compress them into a $ 28 \\times 28 = 784$ -space, you are actually not compressing them\n",
    "    \n",
    "* We can still use this graph of **Loss vs. Latent dimensions** reading it from right to left to decide in which latent space it would be advisable to compress the pictures without losing to much information: `latent_space = 8` seems a sweat spot here using the Elbow Method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6b2CoAHvZ4qC"
   },
   "source": [
    "---\n",
    "\n",
    "üèÅ **Congratulations** üèÅ \n",
    "\n",
    "1. Download this notebook from your `Google Drive` or directly from `Google Colab` \n",
    "2. Drag-and-drop it from your `Downloads` folder to your local challenge folder\n",
    "\n",
    "\n",
    "üíæ Don't forget to push your code\n",
    "\n",
    "3. Follow the usual procedure on your terminal inside the challenge folder:\n",
    "      * *git add autoencoders.ipynb*\n",
    "      * *git commit -m \"I am the god of Transfer Learning\"*\n",
    "      * *git push origin master*\n",
    "\n",
    "*Hint*: To find where this Colab notebook has been saved, click on `File` $\\rightarrow$ `Locate in Drive`.\n",
    "\n",
    "üòâ That was the last challenge of this module!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
